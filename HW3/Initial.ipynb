{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5368ded9-31a9-4865-b433-58b0b06c37c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 14:38:44.394486: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-01 14:38:44.527553: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-01 14:38:45.471180: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib\n",
      "2023-04-01 14:38:45.471248: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib\n",
      "2023-04-01 14:38:45.471252: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizerFast, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c211c4d-7a5b-4695-9572-bb459549923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path): \n",
    "    \n",
    "    with open(path, 'rb') as f:\n",
    "        raw_data = json.load(f)\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_q = 0\n",
    "    num_pos = 0\n",
    "    num_imp = 0\n",
    "    for group in raw_data['data']:\n",
    "        for paragraph in group['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                num_q  = num_q+1\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context.lower())\n",
    "                    questions.append(question.lower())\n",
    "                    answers.append(answer)\n",
    "                    # if answer['is_impossible'] == False:\n",
    "                    #     num_pos = num_pos + 1\n",
    "                    # else:\n",
    "                    #     num_imp = num_imp + 1\n",
    "    return num_q, num_pos, num_imp, contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd39b4a-f8cf-4893-8587-be5441ec26cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_questions = 0\n",
    "num_of_possible = 0\n",
    "num_of_impossible = 0\n",
    "num_q, num_pos, num_imp, train_contexts, train_questions, train_answers = get_data('spoken_train-v1.1.json')\n",
    "num_of_questions  = num_q\n",
    "num_of_possible = num_pos\n",
    "num_of_impossible  = num_imp\n",
    "# print(num_q,num_pos,num_imp)\n",
    "num_q, num_pos, num_imp, valid_contexts, valid_questions, valid_answers = get_data('spoken_test-v1.1.json')\n",
    "# print(num_q,num_pos,num_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44dc7fae-7b57-404f-8cfe-a2bf9ec52243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifies each answer dictionary in place by converting the text to lowercase and adds\n",
    "# key 'answer_end' which represents the ending position of the answer in the context.\n",
    "def add_answer_at_end(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        answer['text'] = answer['text'].lower()\n",
    "        answer['answer_end'] = answer['answer_start'] + len(answer['text'])\n",
    "\n",
    "add_answer_at_end(train_answers, train_contexts)\n",
    "add_answer_at_end(valid_answers, valid_contexts)\n",
    "\n",
    "#  maximum length of the input sequences after tokenization\n",
    "MAX_LENGTH = 250\n",
    "MODEL_PATH = \"bert-base-uncased\"\n",
    "# The tokenizer is used to encode the training and validation questions and contexts into input sequences that can be fed into a BERT model.\n",
    "tokenizerFast = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Encoded dictionaries returned  contain the encoded inputs for the BERT model, including the input IDs, \n",
    "doc_stride = 128 \n",
    "# attention masks, and token type IDs. \n",
    "train_encodings_fast = tokenizerFast(train_questions, train_contexts,  max_length = MAX_LENGTH, truncation=True, padding=True, stride = doc_stride)\n",
    "valid_encodings_fast = tokenizerFast(valid_questions,valid_contexts,  max_length = MAX_LENGTH, truncation=True, padding=True, stride = doc_stride)\n",
    "\n",
    "# The 'input_ids' list contains the token IDs for the encoded input sequence, including special tokens like [CLS] and [SEP]. The input sequence has been padded with zeros to the maximum length specified.\n",
    "\n",
    "# The 'attention_mask' list is a binary mask indicating which tokens in the input sequence are padding tokens (indicated by a value of 0) and which are actual tokens (indicated by a value of 1).\n",
    "\n",
    "# The 'token_type_ids' list is used to distinguish between the question and context portions of the input sequence. For example, in a question-answering task, the question would be represented with 0 values in this list, and the context would be represented with 1 values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83db793-e8b0-40a2-bcb2-a8bdf484090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_Answer_startandend_train(idx):\n",
    "    return_start = 0\n",
    "    return_end = 0\n",
    "    answer_encoding_fast = tokenizerFast(train_answers[idx]['text'],  max_length = MAX_LENGTH, truncation=True, padding=True)\n",
    "    for a in range( len(train_encodings_fast['input_ids'][idx]) -  len(answer_encoding_fast['input_ids']) ): \n",
    "        match = True\n",
    "        for i in range(1,len(answer_encoding_fast['input_ids']) - 1):\n",
    "            if (answer_encoding_fast['input_ids'][i] != train_encodings_fast['input_ids'][idx][a + i]):\n",
    "                match = False\n",
    "                break\n",
    "            if match:\n",
    "                return_start = a+1\n",
    "                return_end = a+i+1\n",
    "                break\n",
    "    return(return_start, return_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae5db2c-866d-4471-aaaf-dbc6eaffd41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "start_positions = []\n",
    "end_positions = []\n",
    "counter = 0\n",
    "for t in range(len(train_encodings_fast['input_ids'])):\n",
    "    s,e = return_Answer_startandend_train(t)\n",
    "    start_positions.append(s)\n",
    "    end_positions.append(e)\n",
    "    if s==0:\n",
    "        counter = counter + 1\n",
    "train_encodings_fast.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "print(counter)\n",
    "print(len(train_encodings_fast))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b237709-027b-4d7e-a1ff-f7b3c60ab5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=250, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings_fast[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fffee-cc34-435f-8cd2-5c6ac5704a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "627aba2d-ca53-4531-86ce-c005975fb82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs:  [101, 2054, 2003, 1999, 2392, 1997, 1996, 10289, 8214, 2364, 2311, 1029, 102, 6549, 2135, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 2082, 8514, 2003, 1996, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1999, 5307, 2009, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 10439, 14995, 6924, 2007, 1996, 5722, 1998, 1996, 2919, 2033, 5004, 3415, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 10047, 2984, 1999, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 2358, 16595, 9648, 4674, 2145, 5255, 7763, 5595, 2809, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 2093, 11342, 1999, 1996, 2751, 8514, 2003, 2004, 3722, 2715, 2962, 6231, 1997, 2984, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokens:  ['[CLS]', 'what', 'is', 'in', 'front', 'of', 'the', 'notre', 'dame', 'main', 'building', '?', '[SEP]', 'architectural', '##ly', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', 'school', 'dome', 'is', 'the', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'in', 'facing', 'it', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'app', '##rai', '##sed', 'with', 'the', 'legend', 'and', 'the', 'bad', 'me', '##ow', 'names', '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'gr', '##otto', 'im', 'mary', 'in', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'gr', '##otto', 'at', 'lou', '##rdes', 'france', 'where', 'the', 'virgin', 'mary', 'reputed', '##ly', 'appeared', 'to', 'st', 'bern', '##ade', '##tte', 'still', 'burning', 'eighteen', 'fifty', 'eight', '.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', 'three', 'statues', 'in', 'the', 'gold', 'dome', 'is', 'as', 'simple', 'modern', 'stone', 'statue', 'of', 'mary', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Offsets:  [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (0, 13), (13, 15), (16, 19), (20, 26), (27, 30), (31, 32), (33, 41), (42, 51), (51, 52), (53, 57), (58, 61), (62, 66), (67, 75), (76, 82), (83, 87), (88, 90), (91, 94), (95, 101), (102, 108), (109, 111), (112, 115), (116, 122), (123, 127), (127, 128), (129, 140), (141, 143), (144, 149), (150, 152), (153, 156), (157, 161), (162, 170), (171, 173), (174, 180), (181, 183), (184, 186), (187, 188), (189, 195), (196, 202), (203, 205), (206, 212), (213, 217), (218, 222), (223, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 252), (253, 256), (257, 260), (261, 263), (263, 265), (266, 271), (271, 272), (273, 277), (278, 280), (281, 284), (285, 289), (290, 298), (299, 301), (302, 305), (306, 314), (315, 317), (318, 321), (322, 328), (329, 334), (334, 335), (336, 347), (348, 354), (355, 358), (359, 367), (368, 370), (371, 374), (375, 377), (377, 381), (382, 384), (385, 389), (390, 392), (393, 398), (399, 401), (402, 408), (409, 412), (413, 423), (423, 424), (425, 427), (428, 430), (431, 432), (433, 440), (441, 443), (444, 447), (448, 450), (450, 454), (455, 457), (458, 461), (461, 465), (466, 472), (473, 478), (479, 482), (483, 489), (490, 494), (495, 502), (502, 504), (505, 513), (514, 516), (517, 519), (520, 524), (524, 527), (527, 530), (531, 536), (537, 544), (545, 553), (554, 559), (560, 565), (565, 566), (567, 569), (570, 573), (574, 577), (578, 580), (581, 584), (585, 589), (590, 595), (596, 599), (600, 602), (603, 604), (605, 611), (612, 616), (617, 621), (622, 630), (631, 638), (639, 644), (645, 652), (653, 655), (656, 659), (660, 664), (665, 669), (670, 672), (673, 675), (676, 682), (683, 689), (690, 695), (696, 702), (703, 705), (706, 710), (710, 711), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
      "Attention Mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Special Tokens Mask:  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Accessing the first record in train_encodings_fast\n",
    "encoding = train_encodings_fast[0]\n",
    "\n",
    "# Accessing the attributes of the encoding\n",
    "ids = encoding.ids\n",
    "tokens = encoding.tokens\n",
    "offsets = encoding.offsets\n",
    "attention_mask = encoding.attention_mask\n",
    "special_tokens_mask = encoding.special_tokens_mask\n",
    "\n",
    "# Printing the results\n",
    "print(\"IDs: \", ids)\n",
    "print(\"Tokens: \", tokens)\n",
    "print(\"Offsets: \", offsets)\n",
    "print(\"Attention Mask: \", attention_mask)\n",
    "print(\"Special Tokens Mask: \", special_tokens_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b900aae-0d57-4a4f-be5f-346406ea1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same for text dataset\n",
    "def return_answer_startend_valid(idx):\n",
    "    return_start = 0\n",
    "    return_end = 0\n",
    "    answer_encoding_fast = tokenizerFast(valid_answers[idx]['text'],  max_length = MAX_LENGTH, truncation=True, padding=True)\n",
    "    for a in range( len(valid_encodings_fast['input_ids'][idx])  -  len(answer_encoding_fast['input_ids'])   ): \n",
    "        match = True\n",
    "        for i in range(1,len(answer_encoding_fast['input_ids']) - 1):\n",
    "            if (answer_encoding_fast['input_ids'][i] != valid_encodings_fast['input_ids'][idx][a + i]):\n",
    "                match = False\n",
    "                break\n",
    "            if match:\n",
    "                return_start = a+1\n",
    "                return_end = a+i+1\n",
    "                break\n",
    "    return(return_start, return_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db2fbbe8-f5d3-4f14-b2b3-90f28727bd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "start_positions = []\n",
    "end_positions = []\n",
    "counter = 0\n",
    "for h in range(len(valid_encodings_fast['input_ids']) ):\n",
    "   \n",
    "    s, e = return_answer_startend_valid(h)\n",
    "    start_positions.append(s)\n",
    "    end_positions.append(e)\n",
    "    if s==0:\n",
    "        counter = counter + 1\n",
    "\n",
    "valid_encodings_fast.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1ad2c4b-9f0d-4226-a0d2-aafcfc0b3fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class InputDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][i]),\n",
    "            'token_type_ids': torch.tensor(self.encodings['token_type_ids'][i]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][i]),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][i]),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][i])\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = InputDataset(train_encodings_fast)\n",
    "valid_dataset = InputDataset(valid_encodings_fast)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "bert_model = BertModel.from_pretrained(MODEL_PATH)  #MODEL_PATH = \"bert-base-uncased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c907021-0f8d-4266-9bda-92b901fdc313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QAModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l1 = nn.Linear(768 * 2, 768 * 2)\n",
    "        self.l2 = nn.Linear(768 * 2, 2)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            self.drop_out,\n",
    "            self.l1,\n",
    "            nn.LeakyReLU(),\n",
    "            self.l2 \n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        model_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        hidden_states = model_output[2]\n",
    "        out = torch.cat((hidden_states[-1], hidden_states[-3]), dim=-1)  \n",
    "        logits = self.linear_relu_stack(out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits\n",
    "\n",
    "model = QAModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3d12a92-4cf1-4f21-8ed2-b330787a44c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdulam/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def loss_function(start_logits, end_logits, start_positions, end_positions):\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)/2\n",
    "    return total_loss\n",
    "\n",
    "def focal_loss_function(start_logits, end_logits, start_positions, end_positions, gamma):\n",
    "    smax = nn.Softmax(dim=1)\n",
    "    probs_start = smax(start_logits)\n",
    "    inv_probs_start = 1 - probs_start\n",
    "    probs_end = smax(end_logits)\n",
    "    inv_probs_end = 1 - probs_end\n",
    "    lsmax = nn.LogSoftmax(dim=1)\n",
    "    log_probs_start = lsmax(start_logits)\n",
    "    log_probs_end = lsmax(end_logits)\n",
    "    \n",
    "    nll = nn.NLLLoss()\n",
    "    \n",
    "    fl_start = nll(torch.pow(inv_probs_start, gamma)* log_probs_start, start_positions)\n",
    "    fl_end = nll(torch.pow(inv_probs_end, gamma)*log_probs_end, end_positions)\n",
    "    return ((fl_start + fl_end)/2)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)\n",
    "scheduler = ExponentialLR(optim, gamma=0.9)\n",
    "total_acc = []\n",
    "total_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ada05c7b-7c07-49bd-a52d-a31fa2a27e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, epoch):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    counter = 0\n",
    "    batch_tracker = 0\n",
    "    for batch in tqdm(dataloader, desc = 'Running Epoch '):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        out_start, out_end = model(input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "        \n",
    "        loss = focal_loss_function(out_start, out_end, start_positions, end_positions,1) \n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        start_predictor = torch.argmax(out_start, dim=1)\n",
    "        end_pred = torch.argmax(out_end, dim=1)\n",
    "            \n",
    "        acc.append(((start_predictor == start_positions).sum()/len(start_predictor)).item())\n",
    "        acc.append(((end_pred == end_positions).sum()/len(end_pred)).item())\n",
    "       \n",
    "        batch_tracker = batch_tracker + 1\n",
    "        if batch_tracker==250 and epoch==1:\n",
    "            total_acc.append(sum(acc)/len(acc))\n",
    "            loss_avg = sum(losses)/len(losses)\n",
    "            total_loss.append(loss_avg)\n",
    "            batch_tracker = 0\n",
    "    scheduler.step()\n",
    "    ret_acc = sum(acc)/len(acc)\n",
    "    ret_loss = sum(losses)/len(losses)\n",
    "    return(ret_acc, ret_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14d23dcc-e65d-4b7a-ad9d-5789d3bca845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    counter = 0\n",
    "    answer_list=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc = 'Running the Evaluation'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_true = batch['start_positions'].to(device)\n",
    "            end_true = batch['end_positions'].to(device)\n",
    "            \n",
    "            out_start, out_end = model(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "            \n",
    "            start_predictor = torch.argmax(out_start)\n",
    "            end_pred = torch.argmax(out_end)\n",
    "            answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_predictor:end_pred]))\n",
    "            tanswer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_true[0]:end_true[0]]))\n",
    "            answer_list.append([answer,tanswer])\n",
    "        \n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffe74a-412d-484d-8696-367609dd4f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8156e86f-61a9-46b3-a3d1-6ea501bd92b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epoch : 100%|██████████| 2320/2320 [06:53<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.613750769711774      Train Loss: 1.137192325281172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running the Evaluation: 100%|██████████| 15875/15875 [01:49<00:00, 144.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.763716535433071]\n",
      "['this', 'is', 'a', 'sentence', '.']\n",
      "this is a sentence.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "wer = load(\"wer\")\n",
    "EPOCHS = 1\n",
    "model.to(device)\n",
    "wer_list=[]\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc, train_loss = train_epoch(model, train_data_loader, epoch+1)\n",
    "    print(f\"Train Accuracy: {train_acc}      Train Loss: {train_loss}\")\n",
    "    answer_list = evaluate_model(model, valid_data_loader)\n",
    "    pred_answers=[]\n",
    "    true_answers=[]\n",
    "    for i in range(len(answer_list)):\n",
    "        if(len(answer_list[i][0])==0):\n",
    "            answer_list[i][0]=\"$\"\n",
    "        if(len(answer_list[i][1])==0):\n",
    "            answer_list[i][1]=\"$\"\n",
    "        pred_answers.append(answer_list[i][0])\n",
    "        true_answers.append(answer_list[i][1])\n",
    "    wer_score = wer.compute(predictions=pred_answers, references=true_answers)\n",
    "    wer_list.append(wer_score)\n",
    "print(wer_list)\n",
    "\n",
    "tokens = tokenizerFast.tokenize(\"This is a sentence.\")\n",
    "print(tokens)\n",
    "output = tokenizerFast.convert_tokens_to_string(tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3008daba-4c3c-4c64-bfdf-67f5045d5db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Save model weights\n",
    "# torch.save(model.state_dict(), 'model_weights.pt')\n",
    "\n",
    "# # Load model weights\n",
    "# model.load_state_dict(torch.load('model_weights.pt'))\n",
    "\n",
    "# # Make "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98a48d71-1a43-46e0-9844-42f56c3ca916",
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"spoken_test-v1.1.json\")\n",
    "test_data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d120dfe3-7278-4633-a7e6-3888d17f495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6460dbf6-485e-4139-a1a6-df839055a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "    \n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c5343a7-2c6e-4bb6-9768-26f5d15f0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "  \n",
    "      # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "  \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "  \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "  \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "  \n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbf8f15f-ef5d-4871-8f63-d7fc8713783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "f1_scores=[]\n",
    "\n",
    "model.eval()\n",
    "# models = [model] + [model.from_pretrained('bert-base-uncased').to(device) for _ in range(3-1)]\n",
    "\n",
    "for i, example in enumerate(test_dataset):\n",
    "    for qa_pair in example['paragraphs']:\n",
    "        context = qa_pair['context']\n",
    "        for qas in qa_pair['qas']:\n",
    "            inputs = tokenizerFast.encode_plus(qas['question'],context, max_length=MAX_LENGTH, return_tensors='pt').to(device)\n",
    "            start_logits = torch.zeros((1, len(context)))\n",
    "            end_logits = torch.zeros((1, len(context)))\n",
    "            #for model in models:f\n",
    "            out_start, out_end = model(**inputs)\n",
    "            start_predictor = torch.argmax(out_start)\n",
    "            end_pred = torch.argmax(out_end)\n",
    "            answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(inputs['input_ids'][0][start_predictor:end_pred]))\n",
    "            \n",
    "            \n",
    "#             outputs = model(**inputs)\n",
    "#             answer_start = torch.argmax(outputs[0])  # get the most likely beginning of answer with the argmax of the score\n",
    "#             answer_end = torch.argmax(outputs[1]) + 1 \n",
    "#             answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "#             \n",
    "            int(normalize_text(answer) == normalize_text(qas['answers'][0]['text']))\n",
    "            f1_scores.append(compute_f1(answer, qas['answers'][0]['text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee237a5c-ab6d-49be-8882-4b8c78d134ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35046021759984486\n"
     ]
    }
   ],
   "source": [
    "overall_f1_score = np.mean(f1_scores)\n",
    "print(overall_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b118138-e131-4b60-9b92-236b284ef6ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (579) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m end_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(context)))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#for model in models:f\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m out_start, out_end \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m start_predictor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(out_start)\n\u001b[1;32m     22\u001b[0m end_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(out_end)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mQAModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, token_type_ids):\n\u001b[0;32m---> 16\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m model_output[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     18\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], hidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1021\u001b[0m     embedding_output,\n\u001b[1;32m   1022\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1031\u001b[0m )\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:236\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    235\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m--> 236\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    237\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[1;32m    238\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (579) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# # Ensemble Predictions\n",
    "# import numpy as np\n",
    "# f1_scores=[]\n",
    "\n",
    "# model.eval()\n",
    "# # models = [model] + [model.from_pretrained('bert-base-uncased').to(device) for _ in range(3-1)]\n",
    "\n",
    "# for i, example in enumerate(test_dataset):\n",
    "#     for qa_pair in example['paragraphs']:\n",
    "#         context = qa_pair['context']\n",
    "#         for qas in qa_pair['qas']:\n",
    "#             inputs = tokenizerFast.encode_plus(qas['question'],context, return_tensors='pt').to(device)\n",
    "#             #inputs = {\n",
    "#                 #'input_ids': torch.tensor(tokenized['input_ids']).unsqueeze(0).to(device),\n",
    "#                 #'attention_mask': torch.tensor(tokenized['attention_mask']).unsqueeze(0).to(device)             \n",
    "#             #}\n",
    "#             start_logits = torch.zeros((1, len(context)))\n",
    "#             end_logits = torch.zeros((1, len(context)))\n",
    "#             #for model in models:f\n",
    "#             out_start, out_end = model(**inputs)\n",
    "#             start_predictor = torch.argmax(out_start)\n",
    "#             end_pred = torch.argmax(out_end)\n",
    "#             answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(inputs['input_ids'][0][start_predictor:end_pred]))\n",
    "            \n",
    "            \n",
    "# #             outputs = model(**inputs)\n",
    "# #             answer_start = torch.argmax(outputs[0])  # get the most likely beginning of answer with the argmax of the score\n",
    "# #             answer_end = torch.argmax(outputs[1]) + 1 \n",
    "# #             answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "# #             \n",
    "#             int(normalize_text(answer) == normalize_text(qas['answers'][0]['text']))\n",
    "#             f1_scores.append(compute_f1(answer, qas['answers'][0]['text']))\n",
    "         \n",
    "# # input_ids = batch['input_ids'].to(device)\n",
    "# #             attention_mask = batch['attention_mask'].to(device)\n",
    "# #             token_type_ids = batch['token_type_ids'].to(device)\n",
    "# #             start_true = batch['start_positions'].to(device)\n",
    "# #             end_true = batch['end_positions'].to(device)\n",
    "            \n",
    "# #             out_start, out_end = model(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "            \n",
    "# #             start_predictor = torch.argmax(out_start)\n",
    "# #             end_pred = torch.argmax(out_end)\n",
    "# #             answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_predictor:end_pred]))\n",
    "# #             tanswer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_true[0]:end_true[0]]))\n",
    "# #             answer_list.append([answer,tanswer])\n",
    "# # overall_f1_score = np.mean(f1_scores)\n",
    "# # print(overall_f1_score)\n",
    "# #         '''\n",
    "# #         start_logits += outputs.start_logits\n",
    "# #         end_logits += outputs.end_logits\n",
    "# #         start_logits /= ensemble_size\n",
    "# #         end_logits /= ensemble_size\n",
    "# #         start_index = torch.argmax(start_logits, dim=1).item()\n",
    "# #         end_index = torch.argmax(end_logits, dim=1).item()\n",
    "# #         answer = context[start_index:end_index]\n",
    "# #         print(f'Example {i+1}: {answer}')\n",
    "# #         '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f0399-48a4-413e-bfc2-17f3f9c21808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d9f5f-e305-4a41-b276-c13af923b288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad660c36-3ec5-4b41-bec5-bdba45efd356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0464f14-2788-4784-aa6c-b03d9892d06d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ba9d2-cb59-49cf-8c22-25ffedb1e220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6800115-9095-4dec-a830-5f3ef5cb4d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a8777-7109-4b65-b020-086c3d8b1d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd91489-1b61-417d-9d6f-4517ed13b8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51272014-a42a-41d1-9826-7eee64e98b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ea52d-d186-4827-a86b-0015b59e6e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad5387-20f5-4e71-8d4e-e493253f6d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5eb328be-6cf4-4f32-95de-8b568da21129",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_loader2 = DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d5b9fbca-313e-46b7-aaca-c7c50bf37ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 993/993 [00:54<00:00, 18.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "T/P\tanswer_start\tanswer_end\n",
      "\n",
      "true\t77\t78\n",
      "pred\t77\t78\n",
      "\n",
      "true\t77\t78\n",
      "pred\t77\t78\n",
      "\n",
      "true\t77\t78\n",
      "pred\t77\t78\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "acc = []\n",
    "start_true = 0\n",
    "end_true = 0\n",
    "for batch in tqdm(valid_data_loader2):\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "\n",
    "        # outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        out_start, out_end = model(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "            \n",
    "        start_pred = torch.argmax(out_start, dim=1)\n",
    "        end_pred = torch.argmax(out_end, dim=1)\n",
    "\n",
    "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
    "        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
    "\n",
    "acc = sum(acc)/len(acc)\n",
    "\n",
    "print(\"\\n\\nT/P\\tanswer_start\\tanswer_end\\n\")\n",
    "for i in range(len(start_true)):\n",
    "    print(f\"true\\t{start_true[i]}\\t{end_true[i]}\\n\"\n",
    "        f\"pred\\t{start_pred[i]}\\t{end_pred[i]}\\n\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352f727-5377-4ba4-8819-73429f186036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e5ee3-a06f-4382-b27e-ae97886754ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464a6cc-0d14-4202-82c0-cc3510754e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2092b6c-da71-4ba2-a45c-0196e5f75525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca69a37c-c11c-4867-8c30-26692e5dfed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
